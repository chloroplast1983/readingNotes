# 06 | 白话容器基础（二）：隔离与限制

## 笔记

### Namespace

`Namespace`技术实际上修改了应用进程看待整个计算机"视图", 即它的"视线"被操作系统做了限制, 只能"看到"某些指定内容.

对于宿主机来说, 这些被**隔离**的进程跟其他进程并**没有太大区别**.

### Docker Engine 和 Hypervisor

`Docker Engine`不像`Hypervisor`那样对应用进程的隔离环境负责, 也不会创建任何实体的"容器", **真正对隔离环境负责的是宿主机操作系统本身**.

![](./img/06_01.jpg)

用户运行在容器里的应用进程, 跟宿主机上的其他进程一样, 都由宿主机操作系统统一管理, 只不过这些被隔离的进程拥有额外设置过的`Namespace`参数. 而`Docker`更多的是旁路式的辅助和管理工作.

#### Hypervisor 缺点

相比`Hypervisor`来负责创建虚拟机. 这个虚拟机是**真实存在的**, 里面必须**运行一个完整的`Guest OS`才能执行用户的进程**. 不可避免地带来了**额外的资源消耗和占用**.

虚拟机自己就需要占用内存. 此外, 用户应用运行在虚拟机里面, 它对宿主机操作系统的调用就不可避免地经过虚拟化软件的拦截和处理, 这本身又是一层性能损耗, 尤其对计算资源, 网络和磁盘`I/O`的损耗非常大.

**使用容器**意味着这些因为虚拟化而带来的性能损耗都是不存在的. 另一方面`Namespace`作为隔离手段的容器并不需要单独的`Guest OS`, 这就使得**容器额外的资源占用几乎可以忽略不计**.

#### 容器优势

"敏捷"和"高性能"

### Namespace 隔离的不彻底

容器只是运行在宿主机上的一种特殊的进程, 那么多个容器之间使用的就还是同一个宿主机的操作系统内核.

很多资源和对象是不能被`Namesapce`化的, 如**时间**.

```
如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改.
```

### 限制

```
比如在容器的1号进程, 在宿主机上是100号进程. 虽然第100号进程表面上被隔离起来, 但是它所能够用到的资源, 却是可以被宿主机上的其他进程占用的.
```

`Linux Cgroups`就是`Linux`内核中用来为进程设置资源限制的一个重要功能.

#### Linux Cgroups

`Linux Control Group`. 限制一个进程组能够使用的资源**上限**, 包括`CPU`, 内存, 磁盘, 网络带宽等等.

可以对进程进行优先级设置, 审计, 以及将进程挂起和恢复等操作.


`Cgroups`给用户暴露出来的操作接口是文件系统, 它以文件和目录的方式组织在操作系统的`/sys/fs/cgroup`路径下.

```
[ansible@sandbox-services ~]$ ls /sys/fs/cgroup/
blkio  cpu  cpuacct  cpu,cpuacct  cpuset  devices  freezer  hugetlb  memory  net_cls  perf_event  systemd
[ansible@sandbox-services ~]$ mount -t cgroup
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/net_cls type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
```

查看某类资源具体可以被限制的方法.

```
[ansible@sandbox-services ~]$ ls /sys/fs/cgroup/cpu
cgroup.clone_children  cgroup.procs          cpuacct.stat   cpuacct.usage_percpu  cpu.cfs_quota_us  cpu.rt_runtime_us  cpu.stat           release_agent  tasks
cgroup.event_control   cgroup.sane_behavior  cpuacct.usage  cpu.cfs_period_us     cpu.rt_period_us  cpu.shares         notify_on_release  system.slice   user.slice
```

这里可以用来限制进程在长度为`cfs_period`的一段时间内, 只能被分配到总量为`cfs_quota`的`CPU`时间.

#### 示例

```
root@ubuntu:/sys/fs/cgroup/cpu$ mkdir container
root@ubuntu:/sys/fs/cgroup/cpu$ ls container/
cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release
cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks

```

我们新创建的`container`目录下, 自动生成该子系统对应的资源限制文件.


在后台执行这样一条脚本:

```
$ while : ; do : ; done &
[1] 226
```

通过查看`top`可见`226`进程把`cpu`跑满.

我们可以通过查看 container 目录下的文件，看到 container 控制组里的 CPU quota 还没有任何限制(即：-1),CPU period 则是默认的`100 ms`(100000 us):

```
$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 
-1
$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us 
100000
```

向 container 组里的 cfs_quota 文件写入20 ms（20000 us）：

```
$ echo 20000 > /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us
```

因为这在没`100ms`的时间里, 被该控制组限制的进程只能使用`20ms`的`CPU`时间, 也就是说这个进程只能使用到`20%`的`CPU`带宽.

把进程的`PID`写入`container`组里的`tasks`文件.

```
$ echo 226 > /sys/fs/cgroup/cpu/container/tasks 
```

我们在用`top`命令查看, 可以看见`CPU`使用率立刻降到了`20%(%Cpu0 : 20.3 us)`

#### Cgroups 子系统

* `blkio`, 为块设备设定`I/O`限制, 一般用于磁盘等设备.
* `cpuset`, 为进程分配单独的`CPU`核和对应的内存节点.
* `memory`, 为进程设定内存使用限制.

#### Linux Cgroups

就是一个子系统目录加上一组资源限制文件的组合.

对于`Docker`等`Linux`容器项目来说, 只需要在每个子系统下面, 为每个容器创建一个控制组(即创建一个新目录), 然后在启动容器进程后, 把这个进程的`PID`填写到对应控制组的`tasks`文件中就可以了.

#### 示例

```
[ansible@web1 ~]$ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash

[ansible@web1 ~]$ cat /sys/fs/cgroup/cpu/docker/b97f74e37b518a33d75df0cb131f4f75a59da02c04128e3681c56fcf659d3e48/cpu.cfs_quota_us
20000
[ansible@web1 ~]$ cat /sys/fs/cgroup/cpu/docker/b97f74e37b518a33d75df0cb131f4f75a59da02c04128e3681c56fcf659d3e48/cpu.cfs_period_us
100000
```

`b97f74e37b518a33d75df0cb131f4f75a59da02c04128e3681c56fcf659d3e48`是`container id`.

可见在`/sys/fs/cgroup/cpu/docker/容器ID`下对`CPU`的限制.

### 总结

**容器只是一种特殊的进程**

**一个正在运行的`Docker`容器, 其实就是一个启用了多个`Linux Namespace`的应用进程, 而这个进程能够使用的资源量, 则受`Cgroups`配置的限制**.

#### 容器是一个"单进程"模型

容器的本质就是一个进程, 用户的应用进程实际上就是容器里`PID=1`的进程, 也是其他后续创建的所有进程的父进程. 在一个容器中, 你没办法同时运行两个不同的应用, 除非实现找到一个公共的`PID=1`的程序来充当两个不同应用的父进程. 使用`systemd`或`supervisord`这样的软件来代替应用本身作为容器的启动进程.

#### 容器设计

希望容器和应用能够**同生命周期**, 这个概念对后续的容器编排非常重要. 不要出现"容器是正常运行的, 但是里面的应用早已经挂了".

### `/proc`

该目录存储的是记录当前内核运行状态的一系列特殊文件, 用户可以通过访问这些文件, 查看系统以及当前正在运行的进程的信息. **这些文件也是`top`指令查看系统信息的主要数据来源**

如果在容器里面执行`top`指令, 就会发现, 它显示的信息是宿主机的`CPU`和内存数据, 不是当前容器的数据.

`/proc`文件系统并不知道用户通过`Cgroups`给这个容器做了什么样的资源限制，即：`/proc`文件系统不了解`Cgroups`限制的存在.

## 扩展

### cpu.cfs_period_us 和 cpu.cfs_quota_us

#### cpu.cfs_period_us

cpu分配的周期(微秒), 默认为`100000`.

#### cpu.cfs_quota_us

表示该control group限制占用的时间(微秒), 默认为`-1`, 表示不限制. 如果设为`50000`,表示占用`50000/10000=50%`的CPU.


### docker 设置 --cpu-period & --cpu-quota

* `--cpu-period`是用来指定容器对`CPU`的使用在多长时间内做一次重新分配.
* `--cpu-quota`是指在这个周期内, 最多可以有多少时间用来泡这个容器.

### docker 设置 --cpu-shares

`--cpu-shares`可以设置`CPU`利用率权重, 默认为`1024`. 可以设置为`2`或者更高(单个 CPU 为 1024，两个为 2048，以此类推).如果设置选项为 0, 则系统会忽略该选项并且使用默认值 1024.

假如一个 1core 的主机运行 3 个 container, 其中一个 cpu-shares 设置为 1024,而其它 cpu-shares 被设置成 512. 当 3 个容器中的进程尝试使用 100% CPU 的时候「尝试使用 100% CPU 很重要，此时才可以体现设置值」, 则设置 1024 的容器会占用 50% 的 CPU 时间. 如果又添加一个 cpu-shares 为 1024 的 container, 那么两个设置为 1024 的容器 CPU 利用占比为 33%, 而另外两个则为 16.5%. 简单的算法就是, **所有设置的值相加, 每个容器的占比就是 CPU 的利用率**, 如果只有一个容器，那么此时它无论设置 512 或者 1024, CPU 利用率都将是 100%. 当然, 如果主机是 3core, 运行 3 个容器, 两个 cpu-shares 设置为 512, 一个设置为 1024, 则此时每个 container 都能占用其中一个 CPU 为 100%.